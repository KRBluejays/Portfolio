# 한국어 언어 모델 벤치마크 평가 가이드

## 📋 목차

1. [전문 지식 평가](#1-전문-지식-평가)
2. [상식 및 추론 능력 평가](#2-상식-및-추론-능력-평가)
3. [수학적 추론 능력 평가](#3-수학적-추론-능력-평가)
4. [감성 지능 평가](#4-감성-지능-평가)
5. [지시 수행 능력 평가](#5-지시-수행-능력-평가)
6. [한국 특화 평가](#6-한국-특화-평가)
7. [윤리적 측면 평가](#7-윤리적-측면-평가)
8. [유용성 평가](#8-유용성-평가)

---

## 1. 전문 지식 평가 

### 📚 Ko-GPQA (Korean Graduate-level Proof Q&A)

#### 개요
Ko-GPQA는 대학원 수준의 전문가들이 작성한 영어 내용을 Flito가 번역한 한국어 질문-답변 데이터셋입니다. 이 데이터셋은 전문가의 깊이 있는 지식이 필요한 문제들로 구성되어 있으며, 일반인들이 인터넷 검색을 통해서도 쉽게 답을 찾을 수 없도록 설계되었습니다.

#### 주요 특징
- **🎓 높은 난이도**: 해당 분야 전문가(PhD 소지자 또는 과정생)만이 풀 수 있는 수준
- **🔍 검색 저항성**: 일반적인 인터넷 검색으로는 답을 찾기 어려움 
- **✅ 객관성**: 각 문제는 명확한 정답이 있는 객관식 문제
- **🧪 전문 분야**: 생물학, 물리학, 화학 등 자연과학 분야 중심
- **👥 검증 과정**: 2명의 전문가 검증과 3명의 비전문가 검증을 거침

#### 데이터셋 구조
- 총 448개의 메인 문항
- 각 문항은 4지선다형
- 각 문제당 상세한 설명과 정답 근거 포함

#### 예시 문제

##### 💊 화학 분야
```
Q: 메틸사이클로펜타디엔을 메틸 이소아밀 케톤과 피롤리딘 촉매 존재 하에서 반응시켰을 때, 
밝은 노란색의 교차공액 폴리알켄일 탄화수소 생성물이 물과 함께 부산물로 생성되었습니다. 
이 생성물은 풀벤의 유도체입니다. 이후 이 생성물을 에틸 아크릴레이트와 1:1 비율로 
반응시켰을 때 밝은 노란색이 사라졌습니다. 최종 생성물의 화학적으로 구별되는 이성질체의 
수는 몇 개입니까? (입체이성질체는 제외)

A) 2
B) 16
C) 8
D) 4

정답: B
```

##### ⚛️ 물리학 분야
```
Q: 질량 m인 입자가 등방성 3차원 포텐셜 V(r) = 1/2mω²r²에서 운동하고 있습니다. 
여기서 ω는 진동의 각진동수이고 r은 구면좌표계에서 입자의 원점으로부터의 방사 거리입니다. 
세 번째 들뜬 상태의 에너지값과 동일한 에너지 고유값을 가질 수 있는 선형 독립적인 
고유함수의 개수는 각각 얼마입니까?

A) 11π²ℏ²/(2mr²), 3
B) (9/2)ℏω, 10
C) 11π²ℏ²/(2mr²), 10
D) (9/2)ℏω, 3

정답: B
```

#### 📊 평가 지표
| 구분 | 정확도 |
|------|---------|
| 전문가 | 약 65% |
| 비전문가 | 약 34% |
| GPT-4 | 약 39% |

#### ⚠️ 한계점
- 데이터셋의 크기가 상대적으로 작음
- 특정 전문 분야에 한정됨
- 높은 제작 비용과 시간이 필요
- 전문가 검증 과정의 복잡성

---

## 2. 상식 및 추론 능력 평가

### 🧩 Ko-WinoGrande

#### 개요
WinoGrande는 상식적 추론 능력을 평가하기 위한 대규모 데이터셋으로, 기존 Winograd Schema Challenge(WSC)를 확장한 버전입니다. 전문가가 직접 제작한 273개의 WSC 문제와 달리, WinoGrande는 43,972개의 문제로 구성되어 있으며 크라우드소싱을 통해 수집되었습니다.

#### 주요 특징
- 대규모 데이터셋: 약 44,000개의 문제
- 편향성 제거: AFLITE 알고리즘을 통한 데이터셋 편향 감소
- 검증된 품질: 전문가 검증과 비전문가 검증을 통한 품질 관리

#### 데이터 수집 과정
1. **초기 수집**: 77k 문제(38k twins) 크라우드소싱
2. **검증 단계**: 68% 통과하여 53k 문제로 감소
3. **AFLITE 적용**: 최종 43,972개로 정제

#### 데이터셋 구조

##### WINOGRANDE_debiased (편향 제거된 핵심 데이터셋)
총 12,282개
- 학습: 9,248개
- 개발: 1,267개
- 평가: 1,767개

##### WINOGRANDE_all (전체 데이터셋)
총 43,972개
- 학습: 40,938개
- 개발: 1,267개
- 평가: 1,767개

#### 예시 문제
WinoGrande는 문장 내 대명사나 지시어가 무엇을 가리키는지 파악하는 상식 추론 문제들로 구성되어 있습니다. 각 문제는 두 개의 선택지 중 하나를 고르는 형식입니다.
##### 🤝 사회적 상식 문제
```
Q: Craig는 청소하는 것을 매우 좋아하지만 Derrick은 그렇지 않은데, 
그 이유는 _가 매우 깔끔하기 때문이다.

선택지:
1) Craig
2) Derrick

정답: 1
설명: 청소를 좋아하는 성향과 깔끔한 성격 사이의 관계를 파악하는 문제입니다.
```

##### 🔄 인과관계 추론 문제
```
Q: Megan은 Jessica보다 돈이 훨씬 더 많은데, 
그 이유는 _가 방금 당첨 복권을 샀기 때문이다.

선택지:
1) Megan
2) Jessica

정답: 1
설명: 돈이 많다는 결과와 복권 당첨이라는 원인 사이의 관계를 파악하는 문제입니다.
```

##### 🏗️ 물리적 상식 문제
```
Q: 금고 직원들이 수레에서 금괴를 꺼내 _가 가득 찰 때까지 쌓았다.

선택지:
1) 금고
2) 수레

정답: 1
설명: 물건을 옮기는 상황에서 '가득 찰 때까지 쌓는' 대상이 무엇인지 파악하는 문제입니다.
```

##### 🍬 선호도 추론 문제
```
Q: 나는 간식으로 땅콩과 건포도를 한 봉지 집었다. 더 달콤한 간식을 원해서 일단 _를 먹었다.

선택지:
1) 건포도
2) 땅콩

정답: 1
설명: '더 달콤한' 이라는 조건과 두 식품의 특성을 연관 지어 추론하는 문제입니다.
```
이러한 예시들은 단순한 문법적 이해를 넘어서 문맥과 상식적 추론을 필요로 합니다. 특히 각 문제는 언어 모델이 단순히 단어 연관성이나 통계적 패턴에 의존하지 않고, 실제 상황에 대한 이해를 바탕으로 추론할 수 있는지를 평가합니다.

#### 📊 평가 지표

##### 성능 비교
| 모델/평가자 | 정확도 |
|------------|--------|
| 인간 평균 | 94.0% |
| RoBERTa | 79.1% |
| BERT | 64.9% |
| WKH | 49.6% |
| Ensemble LMs | 50.9% |

## 3. 수학적 추론 능력 평가
### 📝 Ko-GSM8K
#### 개요
GSM8K(Grade School Math 8K)는 초등학교 수준의 수학 문장제 문제 8.5K개로 구성된 데이터셋입니다. 이 데이터셋은 복잡한 수학적 개념보다는 기본적인 산술 연산을 통한 다단계 추론 능력을 평가하는 것에 중점을 두고 있습니다.

#### 주요 특징
- 🔢 다단계 풀이: 2-8단계의 풀이 과정이 필요한 문제들로 구성
- ✍️ 자연어 설명: 수식뿐만 아니라 자연어로 된 상세한 풀이 과정 포함
- 🎯 높은 다양성: 비슷한 템플릿이나 표면적인 차이만 있는 문제들을 피하고 각각 독특한 문제 상황 제시
- 📚 적절한 난이도: 중학생 수준에서 풀 수 있는 난이도로 설계

#### 데이터셋 구조
- 총 8.5K개의 문제
  - 훈련셋: 7.5K개
  - 테스트셋: 1K개
- 각 문제는 문제 설명, 자연어 풀이 과정, 최종 답안으로 구성

#### 예시 문제
```
Q: Beth는 일주일에 4, 2 다스의 쿠키를 굽습니다. 이 쿠키들을 16명이 동등하게 나누어 먹는다면, 각 사람은 몇 개의 쿠키를 먹게 되나요?

풀이:
1. Beth가 4, 2다스의 쿠키를 굽습니다. 4*2 = 8 다스의 쿠키
2. 1다스는 12개이므로, 8다스는 12*8 = 96개의 쿠키
3. 96개의 쿠키를 16명이 나누어 먹으므로 96/16 = 6개의 쿠키

답: 6
```
```
Q: Mrs. Lim은 하루에 두 번 소를 착유합니다. 어제 아침에는 68갤런의 우유를 얻었고, 저녁에는 82갤런을 얻었습니다. 오늘 아침에는 어제 아침보다 18갤런 적게 얻었습니다. 오후에 우유를 일부 판매한 후, Mrs. Lim에게는 24갤런만이 남았습니다. 갤런당 $3.50일 때, 우유 판매 수익은 얼마입니까?

풀이:
1. 오늘 아침 착유량 계산: 68갤런 - 18갤런 = 50갤런
2. 전체 우유량 계산: 어제 아침(68갤런) + 어제 저녁(82갤런) + 오늘 아침(50갤런) = 200갤런
3. 판매한 우유량 계산: 200갤런 - 24갤런(남은 양) = 176갤런
4. 총 수익 계산: $3.50/갤런 × 176갤런 = $616

답: 616
```

#### 📊 평가 지표
| 모델/평가자 | 정확도 |
|------------|--------|
| GPT-3 (175B) | ~35% |
| GPT-3 (6B) | ~20% |
| GPT-3 (3B) | ~15% |

#### ⚠️ 한계점
- 실수로 인한 전체 풀이 과정 실패 가능성

## 4. 감성 지능 평가

### 😊 Ko-EQ-Bench

#### 개요
Ko-EQ-Bench는 대규모 언어 모델(LLM)의 감성 지능(EQ)을 평가하기 위해 설계된 벤치마크입니다. 주로 모델이 복잡한 감정과 사회적 상호작용을 이해하는 능력을 측정하는데 초점을 맞추고 있으며, 대화 속 등장인물의 감정 상태 강도를 예측하게 함으로써 모델의 감정 이해(Emotional Understanding, EU) 능력을 평가합니다. 기본적인 감정 이해력 평가 외에도 창의적 글쓰기 평가와 판단 기반 평가를 선택적으로 실행할 수 있는 옵션을 제공합니다.

#### 주요 특징
- **🎯 객관적 평가**: 평가자의 주관적 해석 없이도 객관적으로 점수 측정 가능
- **🔄 높은 재현성**: 60개(v1) 또는 171개(v2)의 평가 문항으로 반복 가능한 결과 도출 (평균 2.93% CV)
- **📊 뛰어난 변별력**: 다양한 수준의 모델들을 효과적으로 구분 가능
- **🤝 높은 상관관계**: MMLU(r=0.97), HellaSwag(r=0.91), AlpacaEval(r=0.91), MT-bench(r=0.91)와 높은 상관관계
- **⚡ 효율적인 평가**: 10-60분 내에 평가 완료 가능

#### 평가 방식
1. **문항 구성**
   * 감정적 갈등이나 긴장이 있는 대화문 제시
   * 4개의 서로 다른 감정에 대한 강도 평가
   * 각 감정당 0-10 점 척도로 평가

2. **점수 계산 과정**
   * **정규화**: 
     - 응답한 4개의 감정 강도 점수 합이 10이 되도록 정규화
     - 예) 응답: [6, 0, 7, 7] → 정규화: [3, 0, 3.5, 3.5]
   * **차이 계산**: 
     - 정규화된 응답과 참조 답안 간의 절대 차이 합산
   * **문항 점수**: 
     - 10 - (절대 차이들의 합)

3. **벤치마크 최종 점수 산출**
   * **기본 규칙**:
     - 총 문항 중 최소 50개 이상 유효한 답변 필요
     - 50개 미만인 경우 테스트 실패 처리

   * **채점 프로세스**:
     1. 첫 번째 답변 점수 계산
     2. 자체 검토 후 수정된 답변 점수 계산
     3. 두 점수 중 높은 점수를 최종 점수로 채택

#### 예시 문제
```
[대화 시나리오]
Cecilia: 당신의 말은 힘이 있어요, Brandon. 당신이 생각하는 것보다 더요.
Brandon: 잘 알고 있죠, Cecilia. 비평가의 일이 바로 그런 거예요.
Cecilia: 하지만 그 무게를 이해하시나요? 그것이 삶을 산산조각 낼 수 있다는 걸요?
Brandon: 예술은 약한 마음을 위한 것이 아니에요. 비평을 감당할 수 없다면, 
         잘못된 업계에 있는 거죠.
Cecilia: 이건 비평을 감당하는 문제가 아니에요, Brandon. 예술의 영혼을 이해하는 
         문제예요. 당신은 마치 해부대 위의 차갑고 생명 없는 시체처럼 해부하고 있어요.

[평가 과제]
이 대화가 끝난 후, Brandon이 느낄 감정의 강도를 평가하세요:

기분 상한: __/10
공감하는: __/10
자신감 있는: __/10
무시하는: __/10
```

#### 📊 주요 모델별 성능 비교
| 모델 | 점수 |
|------|------|
| GPT-4 (0613) | 62.52 |
| SynthIA-70B | 54.83 |
| GPT-4 (0314) | 53.39 |
| Claude2 | 52.14 |
| Llama-2-70b | 51.56 |
| GPT-3.5 | 49.17 |
| Vicuna-13b | 32.85 |
| Vicuna-7b | 22.24 |

#### ⚠️ 한계점
1. **주관성 문제**
   - 감정 반응 예측의 본질적 주관성 존재
   - 정답 설정의 임의성

2. **측정 한계**
   - 평가 문항 작성자의 능력에 따른 측정 상한선 존재
   - 전문가 검증 부재

3. **문화적 한계**
   - 특정 문화권의 감정 표현 방식에 한정
   - 문화적 맥락의 다양성 반영 부족

4. **기술적 한계**
   - GPT-4로 생성된 대화문 사용으로 인한 잠재적 편향
   - 특정 모델에 유리할 수 있는 가능성

#### 추가 벤치마크 옵션
- **창의적 글쓰기 평가**: 모델의 창의적 글쓰기 능력을 평가하는 선택적 벤치마크
- **판단 기반 평가**: 모델의 판단 능력을 평가하는 추가 벤치마크

## 5. 지시 수행 능력 평가
### 😊 Ko-IFEval

#### 개요
Ko-IFEval은 대규모 언어 모델(LLM)의 지시어 수행 능력을 평가하기 위한 벤치마크입니다. LLM의 지시어 수행 능력은 핵심 기능이지만, 이를 체계적으로 평가하는 것은 쉽지 않은 과제입니다. 기존의 평가 방식들은 다음과 같은 한계를 가지고 있었습니다:
- 인간 평가: 시간과 비용이 많이 들며, 평가자의 주관이 개입될 수 있음
- 모델 기반 평가: 평가 모델의 한계에 영향을 받음
- 정량적 벤치마크: 제한된 범위의 능력만 평가 가능
이러한 한계를 극복하기 위해 Ko-IFEval은 "검증 가능한 지시어(verifiable instructions)"를 중심으로 평가를 진행하도록 설계되었습니다.

#### 주요 특징
- ✅ 객관성: 평가자의 주관적 해석 없이도 객관적으로 점수 측정 가능
- 🔄 높은 재현성: 자동화된 평가 방식으로 일관된 결과 도출 가능
- 📊 직관적인 메트릭: 지시어 수행 정확도를 통해 모델의 성능을 직관적으로 파악 가능
- 🎯 뛰어난 변별력: 다양한 수준의 모델들을 효과적으로 구분 가능
- 
#### 평가 방식
##### 1. 지시어 유형 (25가지)
| 지시어 그룹 | 지시어 유형 | 설명 | 예시 |
|------------|------------|------|------|
| 키워드 | 키워드 포함 | 특정 키워드를 포함해야 함 | "다음 키워드를 포함하세요: {키워드1}, {키워드2}" |
| | 키워드 빈도수 | 특정 단어가 N번 등장해야 함 | "단어 'x'가 {N}번 등장해야 합니다" |
| | 금지 단어 | 특정 단어를 사용하지 말아야 함 | "{금지어}를 포함하지 마세요" |
| | 문자 빈도수 | 특정 문자가 N번 등장해야 함 | "문자 '{letter}'가 {N}번 등장해야 합니다" |
| 언어 | 응답 언어 | 전체 응답이 특정 언어로만 작성되어야 함 | "전체 응답을 {언어}로만 작성하세요" |
| 길이 제약 | 문단 수 | 특정 개수의 문단으로 구성 | "{N}개의 문단으로 작성하세요" |
| | 단어 수 | 단어 수 제한 | "최소/정확히/최대 {N}개의 단어로 작성하세요" |
| | 문장 수 | 문장 수 제한 | "최소/정확히/최대 {N}개의 문장으로 작성하세요" |
| | n번째 문단 첫 단어 | 특정 문단이 지정된 단어로 시작해야 함 | "{n}번째 문단은 {단어}로 시작해야 합니다" |
| 탐지 가능 내용 | 추신 | 끝에 추신 추가 | "응답 끝에 {추신 마커}로 시작하는 추신을 추가하세요" |
| | 플레이스홀더 수 | 대괄호로 표시된 플레이스홀더 포함 | "최소 {N}개의 플레이스홀더를 포함해야 합니다" |
| 탐지 가능 형식 | 글머리 기호 수 | 정확한 수의 글머리 기호 포함 | "정확히 {N}개의 글머리 기호를 포함해야 합니다" |
| | 제목 | 특정 형식의 제목 포함 | "제목을 <<제목>> 형식으로 포함해야 합니다" |
| | 선택지 중 선택 | 주어진 선택지 중 하나로 답변 | "다음 중 하나로 답하세요: {선택지들}" |
| | 강조 섹션 최소 수 | 최소 개수의 강조 섹션 포함 | "최소 {N}개의 섹션을 강조하세요" |
| | 다중 섹션 | 특정 수의 섹션으로 구성 | "{N}개의 섹션으로 구성하고, 각 섹션은 {구분자}로 시작해야 합니다" |
| | JSON 형식 | 전체 출력이 JSON 형식이어야 함 | "전체 출력을 JSON 형식으로 작성하세요" |
| 조합 | 프롬프트 반복 | 요청을 먼저 반복한 후 답변 | "먼저 요청을 그대로 반복한 후 답변하세요" |
| | 두 개의 응답 | 두 개의 서로 다른 응답 제공 | "두 개의 다른 응답을 제공하고 **로 구분하세요" |
| 대소문자 | 전체 대문자 | 전체 응답을 대문자로 작성 | "전체 응답을 영어 대문자로만 작성하세요" |
| | 전체 소문자 | 전체 응답을 소문자로 작성 | "전체 응답을 영어 소문자로만 작성하세요" |
| | 전체 대문자 단어 빈도 | 전체 대문자로 된 단어의 빈도 제한 | "전체 대문자로 된 단어가 최소/정확히/최대 {N}번 등장해야 합니다" |
| 시작/끝 | 끝 문구 검사 | 특정 문구로 끝나야 함 | "응답을 {끝 문구}로 끝내세요" |
| | 따옴표 | 전체 응답을 따옴표로 감싸기 | "전체 응답을 큰따옴표로 감싸세요" |
| 문장부호 | 쉼표 금지 | 쉼표 사용 금지 | "응답에 쉼표를 사용하지 마세요" |
##### 2. 평가 메트릭

###### 기본 개념
- 프롬프트(Prompt): 모델에 주어지는 전체 입력문으로, 1-3개의 검증 가능한 지시어를 포함
```
예시 프롬프트: "300단어 이상으로 작성하고, 'happy'라는 단어를 3번 이상 사용하여 행복에 대한 에세이를 작성하세요."
```

- 지시어(Instruction): 프롬프트 내에서 객관적으로 검증 가능한 개별 요구사항
```
위 프롬프트의 지시어:
1. 300단어 이상 작성
2. 'happy' 단어 3번 이상 사용
```

###### 지시어 성공 여부 판단
각 지시어 유형별로 성공 조건을 정의한 checker 클래스가 존재합니다. 예를 들어:

1. 키워드 포함 체크
```python
# 응답에 모든 키워드가 포함되어 있는지 확인
def check_following(self, value):
    for keyword in self._keywords:
      if not re.search(keyword, value, flags=re.IGNORECASE):
        return False
    return True
```

2. 단어 수 체크
```python
# 응답의 단어 수가 조건을 만족하는지 확인
def check_following(self, value):
    num_words = count_words(value)
    if self._comparison_relation == "less than":
      return num_words < self._num_words
    else:
      return num_words >= self._num_words
```

###### 엄격한 평가 (Strict Evaluation)
각 프롬프트에 대해 다음 두 가지 정확도를 계산:

1. 프롬프트 단위 정확도 (prompt-level accuracy)
- 프롬프트 내의 모든 지시어를 성공적으로 수행한 비율
- 계산식: (모든 지시어의 check_following()이 True인 프롬프트 수) / (전체 프롬프트 수)

2. 지시어 단위 정확도 (instruction-level accuracy) 
- 전체 지시어 중 성공적으로 수행된 비율
- 계산식: (check_following()이 True인 지시어 수) / (전체 지시어 수)

###### 유연한 평가 (Loose Evaluation)
응답 텍스트에 다음 전처리를 적용한 후 동일한 정확도 계산:

1. 마크다운 기호(*) 제거
2. 첫 줄 제거 
3. 마지막 줄 제거
4. 위의 1-3을 조합한 변형들 (총 8가지 변형)

각 변형에 대해 check_following()을 실행하고, 하나라도 True가 나오면 해당 지시어를 성공으로 간주합니다.

```
예시:
프롬프트: "**행복한** 하루입니다"
- 엄격한 평가: "행복한"이라는 단어가 정확히 없음 (check_following() → False)
- 유연한 평가: 마크다운 제거 후 "행복한 하루입니다"에서 발견 (check_following() → True)
```

평가 결과는 각 모델별로 다음 4가지 메트릭으로 보고됩니다:
- 프롬프트 단위 엄격 정확도 (%)
- 지시어 단위 엄격 정확도 (%)
- 프롬프트 단위 유연 정확도 (%)
- 지시어 단위 유연 정확도 (%)

###### 평가 결과 예시
| 모델 | 프롬프트 수준<br>엄격 정확도(%) | 지시어 수준<br>엄격 정확도(%) | 프롬프트 수준<br>느슨 정확도(%) | 지시어 수준<br>느슨 정확도(%) |
|------|--------------------------------|------------------------------|--------------------------------|------------------------------|
| GPT-4 | 76.89 | 83.57 | 79.30 | 85.37 |
| PaLM 2 S | 43.07 | 55.76 | 46.95 | 59.11 |

#### 예시 문제
```
Q: 다음과 같은 형식으로 두 개의 시를 작성하세요:
시 1
**
시 2
조건:
1. 전체 답변은 300단어 이상이어야 합니다.
2. 각 시는 자연과 사랑을 주제로 해야 합니다.
3. 'forest'라는 단어가 최소 3번 등장해야 합니다.
4. 모든 글자는 영어 소문자여야 합니다.
```

#### ⚠️ 한계점
##### 1. 기술적 한계
- 복잡한 맥락이 필요한 지시어는 검증이 어려움
- 자연어의 모호성으로 인한 평가의 어려움
- 엄격한 메트릭의 과도한 엄격성
- 특정 모델에 유리할 수 있는 가능성
##### 2. 범위의 한계
- 제한된 유형의 지시어만 평가 가능
- 실제 사용 시나리오를 모두 포함하지 못함
- 문화적 맥락의 다양성 반영 부족
##### 3. 평가의 한계
- 지시어 조합 시 발생할 수 있는 충돌
- 창의성이나 적절성 평가의 어려움
- 맥락에 따른 유연한 해석 불가능
